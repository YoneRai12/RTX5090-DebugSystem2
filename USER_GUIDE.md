# RTX5090-DebugSystem ユーザーガイド

このシステムは、**「寝ている間にAIが勝手にバグを直して学習を続けてくれる」** 自動運転ツールです。
特に RTX5090 などの強力なGPUマシンを、**自宅**に置いて無人で回し続けるために設計されています。

## 🛠️ 仕組み（どうやって動くの？）

このシステムは `phoenix_cli_manager.py` というマネージャーが司令塔になります。

1.  **監視 (Monitor)**:
    *   あなたの学習スクリプト（例: `train.py`）を起動し、じっと見守ります。
    *   ログが出なくなったり（ハングアップ）、エラーで落ちたり（クラッシュ）するのを検知します。

2.  **診断 (Analyze)**:
    *   エラーが起きると、そのエラーログとソースコードを LLM (Gemini) に送信します。
    *   「このエラーはどう直せばいい？」と相談します。

3.  **手術 (Shadow Patching)**:
    *   **ここが安全ポイント！** いきなり本番ファイルを書き換えません。
    *   まず「影武者（コピーしたファイル）」に修正を適用します。
    *   その影武者でテストを実行し、本当に直ったか、他を壊していないか確認します。

4.  **適用 & 再開 (Commit & Restart)**:
    *   テストに合格したら、本番ファイルを書き換えます（アトミック更新）。
    *   そして学習を再開します。

これを繰り返すことで、夜中にエラーで止まっても、朝には直って学習が進んでいる状態を目指します。

---

## 🧠 最適化戦略（2つのループ）

このシステムは、Discord Botとして対話しながら自分を賢くするために、2つのループを回します。

1.  **オンライン最適化（即効性）**:
    *   Discord チャット中は**重み更新しません**。
    *   代わりに、あなたの好みや直近の会話を**メモリ**から検索し、プロンプトに注入することで即座に振る舞いを修正します。

2.  **オフライン最適化（じわじわ賢く）**:
    *   重み更新（LoRA学習）は、**夜間にまとめて**行います。
    *   `phoenix_cli_manager.py` が学習プロセスを監視し、エラーが出ても自動修復しながら学習を完遂させます。
    *   更新されたモデルは、テストに合格した時のみ反映されます。

---

## 🚀 使い方（3ステップ）

### ステップ 1: 準備

まず、修正してほしいファイルと、実行コマンドを環境変数で教えます。
PowerShell での設定例です。

```powershell
# 1. 修正を許可するファイル（これ以外は絶対触らせない安全装置）
$env:PHOENIX_ALLOWLIST = "train.py;models/*.py"

# 2. 実行する学習コマンド
$env:PHOENIX_TRAIN_CMD = "python train.py --batch_size 32"

# 3. Gemini の設定 (APIキーがある場合)
$env:PHOENIX_PRIMARY = "gemini_api"
$env:GEMINI_API_KEY = "あなたのAPIキー"
```

### ステップ 2: 起動

マネージャー経由で学習をスタートします。

```powershell
python phoenix_cli_manager.py
```

これだけです！
画面には学習ログが表示され、エラーが起きると自動で「修復モード」に入ります。

### ステップ 3: 無人運転（タスクスケジューラ）

**自宅**で放置する場合は、Windowsの「タスクスケジューラ」を使います。

*   **プログラム**: `python.exe` (フルパス推奨)
*   **引数**: `phoenix_cli_manager.py`
*   **開始オプション (Start in)**: **重要！** このフォルダのパス（`C:\...\RTX5090-DebugSystem`）を必ず入れてください。
*   **セキュリティ設定**: 「ユーザーがログオンしているかどうかにかかわらず実行する」にチェック。

これで、PCが再起動しても勝手に裏で学習が回ります。

---

## 🛡️ 安全機能（なぜ暴走しないの？）

AIにコードを書き換えさせるのは怖いですよね？ だからガチガチの安全装置を付けました。

1.  **テスト保護**: `tests/` フォルダは読み取り専用です。AIがテストを改ざんして「合格しました！」と嘘をつくのを防ぎます。
2.  **変更制限**: 1回に変更できるのは 50行、3ファイルまで。大規模な破壊を防ぎます。
3.  **ロックファイル**: 間違って2回起動しても、2つ目はすぐに終了します。GPUの取り合いになりません。
4.  **OOM対策**: 「メモリ不足 (Out of Memory)」の場合は、1回だけ「バッチサイズ下げてみる？」等の修正を試しますが、それでもダメなら諦めて停止します。無限ループでGPUを痛めるのを防ぎます。

## 📂 ログの見方

*   `.phoenix_cli/run.log`: 全体の動作ログ（JSON形式）。いつエラーが起きて、どう直したかが記録されます。
*   `.phoenix_cli/backups/`: 書き換えられる前のファイルのバックアップがここに残ります。いつでも元に戻せます。

## 🆘 フォールバック機能 (Fallback)

Gemini や Grok などの外部APIがダウンした場合や、ネットワークが切断された場合に、**RTX5090 上のローカルLLM** に自動で切り替えて修正を継続できます。

### 設定方法

#### 1. CLIモード (コマンド実行)
環境変数 `PHOENIX_FALLBACK_LLM_CMD` に、ローカルLLMを呼び出すコマンドを設定します。

```powershell
# 例: ローカルの StarCoder を呼び出すスクリプトを指定
$env:PHOENIX_FALLBACK_LLM_CMD = "python local_llm_client.py --model starcoder"
```

#### 2. APIモード (LM Studio / vLLM / llama.cpp)
OpenAI互換のAPIサーバーがローカルで動いている場合（例: LM Studio）、以下のように設定します。

```powershell
$env:PHOENIX_FALLBACK_LLM_TYPE = "api"
$env:PHOENIX_FALLBACK_LLM_URL = "http://localhost:1234/v1/chat/completions"
$env:PHOENIX_FALLBACK_LLM_MODEL = "local-model" # 必要ならモデル名を指定
```

### 推奨ローカルモデル (RTX5090向け)
*   **DeepSeek Coder 33B**: コーディング性能が非常に高い。48GB VRAMなら余裕。
*   **Code Llama 34B**: Meta製の強力なモデル。
*   **StarCoder2 15B**: 高速で精度が良い。32GB VRAMでも快適。
*   **Phi-3 Medium**: 軽量ながら非常に賢い。

これにより、メインのLLMがエラーを返すと、自動的にローカルLLMが引き継ぎます。
RTX5090 のパワーを活かして、外部依存なしで自律的に修復し続けることが可能です。
